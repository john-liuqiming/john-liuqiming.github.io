[{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/bookmark/","tags":"","title":"收藏"},{"body":" blog       ","categories":"","description":"","excerpt":" blog       ","ref":"/","tags":"","title":"blog"},{"body":"","categories":"","description":"","excerpt":"","ref":"/blog/","tags":"","title":"博客"},{"body":"Primer to Container Security | Linux Journal  Linux Kernel Features a. Namespaces Namespaces ensure the isolation of resources for processes running in a container to that of others. They partition the kernel resources for different processes. One set of processes in a separate namespace will see one set of resources while another set of processes will see another. Processes in different see different process IDs, hostnames, user IDs, file names, names for network access, and some interprocess communication. Hence, each file system namespace has its private mount table and root directory. This isolation can be extended to the child process running within a container. For eg., if there is a PID 1 assigned to a process in a container, the same PID 1 can also be assigned to any other child process using the PID namespace. Similarly, other namespaces like network, mount, PID, User, UTS, IPC, and time could be applied to isolate different resources in a container. However, one limitation with namespaces is that some resources are still not namespace-aware. For example devices.\nb. Control Groups (CGroups) They limit and isolate the resource usage like CPU runtime, memory, disk I/O and network among user-defined groups of processes running on a system. In contrast to namespaces, cgroups limit how many resources can be used, while namespaces control what resources a container can see.\nc. Capabilities Linux implementations distinguish two categories of processes: privileged processes (superuser or root), and unprivileged processes. Privileged processes bypass all kernel permission checks, while unprivileged processes are subject to full permission checking based on the process’s credentials. But in the case of containers, those binary options can be troublesome because providing the whole container to have full root privilege can be dangerous. Capabilities turn this dichotomy into fine-grained access control. A set of capabilities can be assigned to the container which could reduce the container’s root operational threats.\nd. Secure Computation Mode (Seccomp) Seccomp can be used to restrict the actions available within the container. It restricts the process to make some particular predefined syscalls from user space to kernel space. If the process attempts any other system calls, it is terminated by the kernel. A large number of syscalls are exposed to every user-space process, with many of them not required. Restricting them reduces the total kernel surface exposed to the application. seccomp-bpf is an extension to seccomp that allows filtering of system calls using a configurable policy. The combination of restricted and allowed calls are arranged in profiles, and different profiles can be passed to different containers. It provides more fine-grained control than capabilities, giving an attacker a limited number of syscalls from the container during a security compromise.\n Some commonly used LSMs are: SELinux (Security Enhanced Linux) - is the default MAC implementation on RedHat-based Linux Distributions. It is known for being powerful and complex. SELinux is attribute-based which means the security identifiers for files are stored in extended file attributes in the file system. SELinux defines access controls for the applications, processes, and files on a system. It uses security policies, which are a set of rules that tell SELinux what can or can’t be accessed, to enforce the access allowed by a policy. AppArmor - It implements a task-centered policy, with task “profiles” being created and loaded from userspace. Profiles can allow capabilities like network access and raw socket access. AppArmor security policies completely define what system resources individual applications can access, and with what privileges. Tasks on the system that do not have a profile defined run in an unconfined state equivalent to standard Linux DAC permissions.\n   云上一年花掉10万美元，我们决意本地部署私有云_云计算_赵钰莹_InfoQ精选文章  优化云成本方面提出了四点建议，不同企业可以根据自己现阶段情况有条件地加以选择：\n  做到成本可观测。通过建立资源利用率指标和每日对账机制，让企业各部门对成本的认识及管理能力保持一致。\n  公有云物尽其用。企业由于对云产品了解不够、对云成本认识不足，引入公有云从而导致业务复杂度提升、相应成本提高。企业可以通过定时扩缩容和机型降配两种手段，充分利用公有云的弹性和退改灵活性，直接有效降低公有云的使用成本。\n  充分利用弹性与共享。企业可以利用 Kubernetes 切割现有资源并以固定 IP 形式交付资源，并对流量模型进行抽象，结合具体业务建立自动扩缩容和错峰调度等。\n  异地部署、混合编排和在离线整合。这些方式需要从企业整体层面考虑，通常不是一个部门能解决的。企业内部可以公司名义成立专项来实施，先从小业务入手，待验证平台功能完善后，再考虑逐步把核心业务迁移到平台，最终达到优化企业整体资源利用率的目的。\n     ","categories":"","description":"","excerpt":"Primer to Container Security | Linux Journal  Linux Kernel Features a. …","ref":"/blog/2022/05/06/%E5%A4%96%E7%BD%91-2022-05-06/","tags":"","title":"外网 2022-05-06"},{"body":"Taints and Tolerations | Kubernetes  The default value for operator is Equal. A toleration “matches” a taint if the keys are the same and the effects are the same, and:\n the operator is Exists (in which case no value should be specified), or the operator is Equal and the values are equal.     ","categories":"","description":"","excerpt":"Taints and Tolerations | Kubernetes  The default value for operator is …","ref":"/blog/2022/04/29/%E5%A4%96%E7%BD%91-2022-04-29/","tags":"","title":"外网 2022-04-29"},{"body":"Assigning Pods to Nodes | Kubernetes  nodeName  nodeName is a more direct form of node selection than affinity or nodeSelector. nodeName is a field in the Pod spec. If the nodeName field is not empty, the scheduler ignores the Pod and the kubelet on the named node tries to place the Pod on that node. Using nodeName overrules using nodeSelector or affinity and anti-affinity rules. Some of the limitations of using nodeName to select nodes are:\n If the named node does not exist, the Pod will not run, and in some cases may be automatically deleted. If the named node does not have the resources to accommodate the Pod, the Pod will fail and its reason will indicate why, for example OutOfmemory or OutOfcpu. Node names in cloud environments are not always predictable or stable. Here is an example of a Pod spec using the nodeName field:  apiVersion:v1kind:Podmetadata:name:nginxspec:containers:- name:nginximage:nginxnodeName:kube-01The above Pod will only run on the node kube-01.\n   ","categories":"","description":"","excerpt":"Assigning Pods to Nodes | Kubernetes  nodeName  nodeName is a more …","ref":"/blog/2022/04/28/%E5%A4%96%E7%BD%91-2022-04-28/","tags":"","title":"外网 2022-04-28"},{"body":"containerd/PLUGINS.md at main · containerd/containerd  V2 Runtimes The runtime v2 interface allows resolving runtimes to binaries on the system. These binaries are used to start the shim process for containerd and allows containerd to manage those containers using the runtime shim api returned by the binary.\n   containerd/README.md at main · containerd/containerd  When a user specifies a runtime name, io.containerd.runc.v1, they will specify the name and version of the runtime. This will be translated by containerd into a binary name for the shim. io.containerd.runc.v1 -\u003e containerd-shim-runc-v1 containerd keeps the containerd-shim-* prefix so that users can ps aux | grep containerd-shim to see running shims on their system.\n   aws-samples/eks-spark-benchmark: Performance optimization for Spark running on Kubernetes  TPC-DS Benchmark Created by a third-party committee, TPC-DS is the de-facto industry standard benchmark for measuring the performance of decision support solutions. According to its own homepage ( https://www.tpc.org/tpcds/), it defines decision support systems as those that examine large volumes of data, give answers to real-world business questions, execute SQL queries of various operational requirements and complexities (e.g., ad-hoc, reporting, iterative OLAP, data mining), and are characterized by high CPU and IO load.\n   ","categories":"","description":"","excerpt":"containerd/PLUGINS.md at main · containerd/containerd  V2 Runtimes The …","ref":"/blog/2022/04/27/%E5%A4%96%E7%BD%91-2022-04-27/","tags":"","title":"外网 2022-04-27"},{"body":"ATA - 文章详情页  机密计算通过在基于硬件的可信执行环境（TEE）中执行计算的方式来保护使用中的数据。 这些安全的、隔离的环境可防止对使用中的应用程序和数据进行未经授权的访问或篡改，从而提升相关组织在管理敏感数据和受监管数据方面的安全级别。\n   ","categories":"","description":"","excerpt":"ATA - 文章详情页  机密计算通过在基于硬件的可信执行环境（TEE）中执行计算的方式来保护使用中的数据。 这些安全的、隔离的环境可防止对 …","ref":"/blog/2022/04/24/%E5%A4%96%E7%BD%91-2022-04-24/","tags":"","title":"外网 2022-04-24"},{"body":"阿里云容器如何实现 1000Pod/min 一键启动_云原生_李慧文_InfoQ精选文章  云上的容器企业用户可分为四类：共振型、毛刺型、混部型、平稳型  云上容器用户的痛点我们整体上可以归纳为五类：弹性能力不足；成本和资源无法平衡；安全性不足；容灾、稳定性能力不足；性能或异构管理复杂度过高。\n   ","categories":"","description":"","excerpt":"阿里云容器如何实现 1000Pod/min 一键启动_云原生_李慧文_InfoQ精选文章  云上的容器企业用户可分为四类：共振型、毛刺型、混 …","ref":"/blog/2022/04/20/%E5%A4%96%E7%BD%91-2022-04-20/","tags":"","title":"外网 2022-04-20"},{"body":"Go Runtime 设计：计算资源调度_Go_张旭海_InfoQ写作平台  到底要同时执行多少个任务（线程数），才能最大化的利用计算资源呢？《Java 并发编程实战》中给出了如下公式： 显然，基于资源最大化考虑，我们期望 Ucpu→1。 那么，对于计算密集型任务，随着计算占比的不断提高，其 CW→0，因此 Nthreads→Ncpu ；而对于 I/O 密集型任务，随着 I/O 等待占比的不断提高，其 CW→∞ ，因此 Nthreads→∞。\n 前面讨论完后，我们发现通过事件驱动 + 异步 I/O + 优雅切换上下文的办法，可以比较高效且友好的将应用逻辑中的 CPU 处理部分和 I/O 处理部分分开来执行，同时还不降低代码逻辑的完整性。\n此时此刻，只剩下如下的两个问题未能解决：\n  饥饿问题：某些任务由于各种原因，长时间占据 CPU 时间，导致其他任务饥饿，可能产生严重的不公平。\n  线程管理问题：I/O 线程池如何分配更合理；并行的任务之间，如何通信和处理数据竞争。\n  对于问题 1，需要引入抢占式调度，在合适的时机对任务触发抢占，强制该任务出让 CPU。对于问题 2，可以抽象线程管理层，向下管理系统线程，向上提供任务之间的并发原语。\n 基本调度理论  调度，就是分配_资源（resource）_用以执行_任务（task）_的动作。\n这里的资源，可以是计算资源如 CPU，存储资源如内存空间，网络资源如带宽、端口等。任务是基于资源所执行的动作，它依赖资源并通过操作资源来产生价值。\n调度目标\n根据不同的资源、任务以及业务目标，调度器的设计目标是多样的：\n  最大化吞吐量：效率优先，目的是让任务能尽可能充足的利用资源，而不是把资源花费在调度或等待上。\n  最小化等待时间：体验优先，目的是让尽可能多的任务开始执行，效率和任务的实际完成时间次要考虑。\n  最小化延迟和响应时间：体验优先，目的是尽可能让每一个任务都等待相对较少的时间，且能相对较快的执行。\n  最大化公平：公平优先，目的是结合任务的任务优先级，以及单位资源的负载率，尽可能公平的将资源和任务匹配。\n  显然，上述目标之间非但不相辅相成，反而很可能相互掣肘（比如最大吞吐和最小延迟），因此选定调度器的设计目标必须结合实际的业务目标。\n   ","categories":"","description":"","excerpt":"Go Runtime 设计：计算资源调度_Go_张旭海_InfoQ写作平台  到底要同时执行多少个任务（线程数），才能最大化的利用计算资源 …","ref":"/blog/2022/04/15/%E5%A4%96%E7%BD%91-2022-04-15/","tags":"","title":"外网 2022-04-15"},{"body":"Firecracker internals: a deep dive inside the technology powering AWS Lambda · Tal Hoffman  Amazon decided to come-up with a better solution for its serverless workloads requiring:\n Consistent, close-to-native performance, which is also not being affected by other functions running on the same node Functions must be strongly isolated and protected against information disclosure, privilege escalation, and other security risks Full compatibility so functions are able to run arbitrary libraries and binaries without any re-compilation or code changes High and flexible scalability allowing thousands of functions to run on a single machine Functions must be able to over-commit resources, only using the minimal amount of resources they need Startup \u0026 tear-down should be very quick so that functions’ cold-start times remain small   Firecracker takes advantage of the 64-bit Linux Boot Protocol, which specifies how the kernel image should be loaded and run. FC directly boots the Kernel at the protected-mode entry point rather than starting off from the 16-bit real mode.\n Firecracker directly uses the uncompressed kernel image vmlinux, saving additional costs of going through the traditional boot sequence in which the kernel decompresses itself at startup. All of this special FC boot sequence described above enables a major performance boost ultimately resulting in what AWS Lambda customer experience as fast cold starts.\n Note that there’s an even better virtio backend implementation called vhost, which introduces in-kernel virtio devices for KVM featuring direct guest-kernel-to-host-kernel data plane, saving redundant host userspace to kernel space syscalls. Firecracker does not currently use this implementation.\n Virtio specifies 3 possible transport layers offering slightly different layouts \u0026 implementations of those drivers \u0026 devices:\n PCI Bus based transport Memory Mapped IO based transport (FC’s chosen transport) Channel I/O based transport   KVM_IOEVENTFD This ioctl attaches or detaches an ioeventfd to a legal pio/mmio address within the guest. A guest write in the registered address will signal the provided event instead of triggering an exit. Link\n Overall, the flow of registering a MMIO-based device is as follows:\n FC allocates a new slot for the MMIO device It subscribes to the guest-triggered ioevents It registers an irqfd in order to be able to send interrupts to the guest Inserts the device at the MMIO slot And finally sets the kernel bootparams to include the guest driver:     ","categories":"","description":"","excerpt":"Firecracker internals: a deep dive inside the technology powering AWS …","ref":"/blog/2022/04/13/%E5%A4%96%E7%BD%91-2022-04-13/","tags":"","title":"外网 2022-04-13"},{"body":"这三份白皮书里，藏着企业选型数据库的关键_云原生_张俊宝_InfoQ精选文章  面对金融企业复杂的业务体系，单一的数据库往往难以支持运转，几十到上百套应用体系按需选用多样的数据库是常态。\n 在数据库层面，稳定性、安全性和强一致性是金融企业对于数据库产品的首要要求。交易与结算支持、信息查询展示、多维度运营分析、智慧网点和反欺诈等风险管理作为金融行业的关键金融场景，应用了大量数据库产品。\n   ","categories":"","description":"","excerpt":"这三份白皮书里，藏着企业选型数据库的关键_云原生_张俊宝_InfoQ精选文章  面对金融企业复杂的业务体系，单一的数据库往往难以支持运转，几 …","ref":"/blog/2022/04/12/%E5%A4%96%E7%BD%91-2022-04-12/","tags":"","title":"外网 2022-04-12"},{"body":"数据库技术新版图-Serverless数据库_架构_张雅文_InfoQ精选文章  尽管云上数据库能够提供一些监控信息，但在多数场景下，工作负载是不均衡的，波峰和波谷往往差异极大，那么在这样的情况下该如何进行数据库选型呢？一般来说，有以下几个方式：\n第一，为了避免数据库成为瓶颈，开发者可以按照波峰的方式进行部署。但工作负载不是始终都处于波峰，如果统一按照波峰位置部署数据库，就会带来资源浪费，提升成本。\n第二，开发者可考虑按照波峰波谷的工作负载，配置一个平均值。这样成本的确有所节约，但问题是，一旦工作负载达到波峰，数据库将成为瓶颈，严重影响终端用户的体验。\n第三，也是开发者现阶段最为常用的方式，即对不同指标进行监控，设置预警，比如设置当监测到 CPU 利用率到达 80% 的时候，系统发送告警信息，然后由开发或运维人员手动对数据库容量进行调整。尽管这样的方式的确可行，但却会耗费大量的时间成本。\n 最初在 SaaS 应用的数据库实现上，对于多租户的数据管理来说，每个租户的数据都是单独放在一个数据库里的，因此每个租户都会占用一个数据库，这样就会产生成千上万个数据库，成本高昂。\n针对这一问题，当时的解决办法是将多个用户的数据库部署同一个 Aurora 集群来提高利用率和成本效率，这样一定程度上能够解决多租户 SaaS 应用研发的痛点，但会牺牲单个数据用户数据库操作的粒度。 而采用 Serverless 数据库，可辅助进行多租户的 SaaS 应用开发，把每个租户对应到一个 Serverless 数据库，随着应用的变化，可对每个租户数据库的容量进行自动收缩或扩展。\n 最初在 SaaS 应用的数据库实现上，对于多租户的数据管理来说，每个租户的数据都是单独放在一个数据库里的，因此每个租户都会占用一个数据库，这样就会产生成千上万个数据库，成本高昂。\n针对这一问题，当时的解决办法是将多个用户的数据库部署同一个 Aurora 集群来提高利用率和成本效率，这样一定程度上能够解决多租户 SaaS 应用研发的痛点，但会牺牲单个数据用户数据库操作的粒度。 而采用 Serverless 数据库，可辅助进行多租户的 SaaS 应用开发，把每个租户对应到一个 Serverless 数据库，随着应用的变化，可对每个租户数据库的容量进行自动收缩或扩展。\n   ","categories":"","description":"","excerpt":"数据库技术新版图-Serverless数据库_架构_张雅文_InfoQ精选文章  尽管云上数据库能够提供一些监控信息，但在多数场景下，工作负 …","ref":"/blog/2022/04/06/%E5%A4%96%E7%BD%91-2022-04-06/","tags":"","title":"外网 2022-04-06"},{"body":"函数计算基于安全容器全链路性能优化 FY21总结  \nAPI Server接收到函数执行请求后，请求调度器EERouter向EE申请一个容器以执行该请求，EEAgent负责在自己的机器上创建一个容器，并返回之；\nAPI Server获取到容器后，通过EEAgent向容器发起调用函数请求，EEAgent将调用结果转发至容器以执行函数，执行完成后EEAgent将函数执行结果返回至API Server；\nEEAgent收集函数容器内用户业务逻辑产生的日志（函数内调用print或者FC的日志库方式打印到容器标准输出或者标准错误输出stdout/stderr的日志）进行收集，在这里EEAgent是通过docker attach的方式收集容器标准输出日志，并上传到用户配置的SLS上；\n除了需要收集用户的业务逻辑日志，还需要收集FC Runtime在运行过程中产生的日志，主要包括基础系统日志以及函数运行错误的异常信息，并上传到系统Logstore中；\n收集函数运行过程中的metrics数据，包括内存最大使用量，函数运行的计费时间等等，并将此结果返回给函数调用者。\n\n   函数计算冷启动优化之代码加载  冷启动速度是Serverless平台的核心竞争力之一，极致的冷启动可以让业务无需提前预留实例，无需提前预热资源，完全做到按量使用，将资源利用率提升到最高。\n 完全冷启动\n\n准备用户代码：准备代码，以当前使用的最多的Zip类型代码压缩包为例；\na. 下载代码：下载用户代码到宿主机计算节点；\nb. 解压代码，将zip解压至特定目录；\nc. sync code：将解压后的代码通过系统调用sync将强制写入磁盘；\nd. 挂载代码目录至容器：将解压后的代码目录通过bind mount方式挂载到用户容器中。\n\n2. 生成容器配置：根据函数配置，例如网络、cpu、内存等函数配置，生成相应的容器配置，以便后续创建、启动容器；\n3. 创建及启动容器：根据上述配置创建函数运行所需的容器；\n4. 启动函数运行时环境：启动函数运行时runtime，并等待runtime server ready以接收函数请求。\n 我们的目标是：\n单机高密部署：单机高密部署要求在单台神龙机器上可以进行高密部署，并不影响容器正常运行调用。\n单机并发创建：单机并发创建能力，例如要求1秒内拉起数百个用户容器。\n单机超高TPS：瞬间处理数千函数调用请求打到同一台机器上的能力。\n   如虎添翼！高德地图+Serverless 护航你的假日出行-阿里云开发者社区  完全弹性\n请求毫秒级的调度是 Serverless 的核心竞争力，相比传统的分钟级弹性扩容，Serverless 技术存在巨大的成本优势，扩容所耗费的时间越少，预留的机器资源就会更低，如果到了毫秒级别，就无需预留任何资源，这样成本能够大大的降低，资源利用率可以达到 100%。\n   Serverless 应用引擎 SAE 携手谱尼测试共同抗疫-阿里云开发者社区  面对疫情的不断反复，核酸检测预约系统每天都经历着业务洪峰，谱尼测试面临着三大痛点：\n\n运维成本高：面对业务洪峰时每一次都要提前进行容量预估、准备环境、部署应用等繁琐操作，存在大量的重复工作。\n\n应对业务洪峰能力不足：面对突然的流量激增，往往需要临时部署应用进行应对，整个流程不仅耗时，同时影响客户侧的用户体验。\n\n版本迭代风险大：系统上线、版本迭代流程需要一套完整的解决方案，每次上线新的版本都需要进行繁琐的配置来实现发布，并且无法保证发布之后的稳定性。\n\n\n   Netflix/flamescope: FlameScope is a visualization tool for exploring different time ranges as Flame Graphs.  FlameScope is a visualization tool for exploring different time ranges as Flame Graphs, allowing quick analysis of performance issues such as perturbations, variance, single-threaded execution, and more.\nFlameScope begins by displaying the input data as an interactive subsecond-offset heat map. This shows patterns in the data. You can then select a time range to highlight on different patterns, and a flame graph will be generated just for that time range.\n   ","categories":"","description":"","excerpt":"函数计算基于安全容器全链路性能优化 FY21总结  \nAPI Server接收到函数执行请求后，请求调度器EERouter向EE申请一个容器 …","ref":"/blog/2022/04/04/%E5%A4%96%E7%BD%91-2022-04-04/","tags":"","title":"外网 2022-04-04"},{"body":"函数计算冷启动优化之代码加载  冷启动速度是Serverless平台的核心竞争力之一，极致的冷启动可以让业务无需提前预留实例，无需提前预热资源，完全做到按量使用，将资源利用率提升到最高。\n 完全冷启动  准备用户代码：准备代码，以当前使用的最多的Zip类型代码压缩包为例；\na. 下载代码：下载用户代码到宿主机计算节点；\nb. 解压代码，将zip解压至特定目录；\nc. sync code：将解压后的代码通过系统调用sync将强制写入磁盘；\nd. 挂载代码目录至容器：将解压后的代码目录通过bind mount方式挂载到用户容器中。\n2. 生成容器配置：根据函数配置，例如网络、cpu、内存等函数配置，生成相应的容器配置，以便后续创建、启动容器；\n3. 创建及启动容器：根据上述配置创建函数运行所需的容器；\n4. 启动函数运行时环境：启动函数运行时runtime，并等待runtime server ready以接收函数请求。   我们的目标是：\n单机高密部署：单机高密部署要求在单台神龙机器上可以进行高密部署，并不影响容器正常运行调用。\n单机并发创建：单机并发创建能力，例如要求1秒内拉起数百个用户容器。\n单机超高TPS：瞬间处理数千函数调用请求打到同一台机器上的能力。\n   函数计算基于安全容器全链路性能优化 FY21总结   API Server接收到函数执行请求后，请求调度器EERouter向EE申请一个容器以执行该请求，EEAgent负责在自己的机器上创建一个容器，并返回之； API Server获取到容器后，通过EEAgent向容器发起调用函数请求，EEAgent将调用结果转发至容器以执行函数，执行完成后EEAgent将函数执行结果返回至API Server； EEAgent收集函数容器内用户业务逻辑产生的日志（函数内调用print或者FC的日志库方式打印到容器标准输出或者标准错误输出stdout/stderr的日志）进行收集，在这里EEAgent是通过docker attach的方式收集容器标准输出日志，并上传到用户配置的SLS上； 除了需要收集用户的业务逻辑日志，还需要收集FC Runtime在运行过程中产生的日志，主要包括基础系统日志以及函数运行错误的异常信息，并上传到系统Logstore中； 收集函数运行过程中的metrics数据，包括内存最大使用量，函数运行的计费时间等等，并将此结果返回给函数调用者。     ","categories":"","description":"","excerpt":"函数计算冷启动优化之代码加载  冷启动速度是Serverless平台的核心竞争力之一，极致的冷启动可以让业务无需提前预留实例，无需提前预热资 …","ref":"/blog/2022/04/01/%E5%A4%96%E7%BD%91-2022-04-01/","tags":"","title":"外网 2022-04-01"},{"body":"Netflix/flamescope: FlameScope is a visualization tool for exploring different time ranges as Flame Graphs.  FlameScope is a visualization tool for exploring different time ranges as Flame Graphs, allowing quick analysis of performance issues such as perturbations, variance, single-threaded execution, and more. FlameScope begins by displaying the input data as an interactive subsecond-offset heat map. This shows patterns in the data. You can then select a time range to highlight on different patterns, and a flame graph will be generated just for that time range.\n   Serverless 应用引擎 SAE 携手谱尼测试共同抗疫-阿里云开发者社区  面对疫情的不断反复，核酸检测预约系统每天都经历着业务洪峰，谱尼测试面临着三大痛点：\n 运维成本高：面对业务洪峰时每一次都要提前进行容量预估、准备环境、部署应用等繁琐操作，存在大量的重复工作。 应对业务洪峰能力不足：面对突然的流量激增，往往需要临时部署应用进行应对，整个流程不仅耗时，同时影响客户侧的用户体验。 版本迭代风险大：系统上线、版本迭代流程需要一套完整的解决方案，每次上线新的版本都需要进行繁琐的配置来实现发布，并且无法保证发布之后的稳定性。     如虎添翼！高德地图+Serverless 护航你的假日出行-阿里云开发者社区  完全弹性 请求毫秒级的调度是 Serverless 的核心竞争力，相比传统的分钟级弹性扩容，Serverless 技术存在巨大的成本优势，扩容所耗费的时间越少，预留的机器资源就会更低，如果到了毫秒级别，就无需预留任何资源，这样成本能够大大的降低，资源利用率可以达到 100%。\n   ","categories":"","description":"","excerpt":"Netflix/flamescope: FlameScope is a visualization tool for exploring …","ref":"/blog/2022/03/31/%E5%A4%96%E7%BD%91-2022-03-31/","tags":"","title":"外网 2022-03-31"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"搜索结果"}]